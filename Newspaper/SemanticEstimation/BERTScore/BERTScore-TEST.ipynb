{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d780dff3",
   "metadata": {},
   "source": [
    "## BERTScore Tutorial\n",
    "### Installation\n",
    "\n",
    "if you have not installed `bert_score`, it is very easy simply uncomment the line below to install through pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322a0db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.12'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your installation\n",
    "import bert_score\n",
    "bert_score.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98f59c",
   "metadata": {},
   "source": [
    "### preparation\n",
    "\n",
    "Note, we are now using the practice of centralising python libraries specific to this set of Jupyter Notebooks in one location. This is possible because Python uses the concept of module naming and location as shown in the graphic below\n",
    "<div>\n",
    "   <img src=\"../../../images/SysPath.jpg\" height=\"600\"; width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1882e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path : ['/home/deleidos/Notebooks/Newspaper/SemanticEstimation/BERTScore', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/home/deleidos/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "LIBRARY_PATH = '/home/deleidos/Notebooks/Newspaper/SemanticEstimation'\n",
    "LIBRARY_PATH = \"/home/deleidos/Notebooks/lib\"\n",
    "print (f\"sys.path : {sys.path}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = os.getenv('PATH')\n",
    "# print (path)\n",
    "\n",
    "# os.environ['PATH'] =  path + \":..\"\n",
    "# print (os.getenv('PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdf775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path : ['/home/deleidos/Notebooks/lib', '/home/deleidos/Notebooks/Newspaper/SemanticEstimation/BERTScore', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/home/deleidos/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "try:\n",
    "    sys.path.remove(LIBRARY_PATH )\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "# try:\n",
    "#     sys.path.remove('../')\n",
    "# except ValueError:\n",
    "#     pass\n",
    "\n",
    "sys.path.insert(0, LIBRARY_PATH )\n",
    "# sys.path.insert(0, '../' )\n",
    "print (f\"sys.path : {sys.path}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613f5f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deleidos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !ls ..\n",
    "from dynamoDButils import *\n",
    "from APIutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22da832",
   "metadata": {},
   "source": [
    "This version of the experiment uses our Corpus of articles as reference and candidates from a set of sites we choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09509052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell imports done @ 2023-01-25 20:30:49\n"
     ]
    }
   ],
   "source": [
    "# hide the loading messages\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# remember to put in an empty __init__.py in the parent folder\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from dynamoDButils import *\n",
    "from APIutils import *\n",
    "from seedterms import *\n",
    "\n",
    "import time\n",
    "from datetime import date, datetime \n",
    "import datetime\n",
    "from pathlib import Path \n",
    "import os \n",
    "\n",
    "def complete(state=\"complete\"):\n",
    "    print(f\"\\nCell {state} @ {(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))}\")\n",
    "\n",
    "complete(state='imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"xtick.major.size\"] = 0\n",
    "rcParams[\"xtick.minor.size\"] = 0\n",
    "rcParams[\"ytick.major.size\"] = 0\n",
    "rcParams[\"ytick.minor.size\"] = 0\n",
    "\n",
    "rcParams[\"axes.labelsize\"] = \"large\"\n",
    "rcParams[\"axes.axisbelow\"] = True\n",
    "rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab629f8",
   "metadata": {},
   "source": [
    "## Function API\n",
    "\n",
    "We will first demonstrate how to use the `score` function in `bert_score`, which is what you need to evaluate a set of machine generated outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da766b",
   "metadata": {},
   "source": [
    "Inputs to `score` are a list of candidate sentences and a list of reference sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ddb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hyps.txt\") as f:\n",
    "    cands = [line.strip() for line in f]\n",
    "\n",
    "with open(\"refs.txt\") as f:\n",
    "    refs = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78daa1",
   "metadata": {},
   "source": [
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ff187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"candidate [0]: {cands[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477c642",
   "metadata": {},
   "source": [
    "We are now ready to call the score function. Besides candidates and references, we need to speicify the bert model we are using. Since we are dealing with English sentences, we will use the *bert-base-uncased* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f274c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad1865",
   "metadata": {},
   "source": [
    "The outputs of the `score` function are Tensors of precision, recall, and F1 respectively. Each Tensor has the same number of items with the candidate and reference lists. Each item in the list is a scalar, representing the score for the corresponding candidates and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa2abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff5e88",
   "metadata": {},
   "source": [
    "We can take the average of all candidate reference pairs to be the system level score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b308710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76575eab",
   "metadata": {},
   "source": [
    "It might also be very interestig to see the distribution of BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95829128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1, bins=10)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf08782",
   "metadata": {},
   "source": [
    "Some contextual embedding models, like RoBERTa, often produce BERTScores in a very narrow range (as shown above, the range is roughly between 0.92 and 1). Although this artifact does not affect the ranking ability of BERTScore, it affects the readability. Therefore, we propose to apply \"baseline rescaling\" to adjust the output scores. More details on this feature can be found in [this post](https://github.com/Tiiiger/bert_score/blob/master/journal/rescale_baseline.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score(cands, refs, lang='en', rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8592666",
   "metadata": {},
   "source": [
    "We can now see that the scores are much more spread out, which makes it easy to compare different examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40517b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce809f14",
   "metadata": {},
   "source": [
    "The `score` function also handles multiple references gracefully. Consider a candidate sentences with 3 references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cands = ['I like lemons.']\n",
    "multi_refs = [['I am proud of you.', 'I love lemons.', 'Go go go.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_mul, R_mul, F_mul = score(single_cands, multi_refs, lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e1cc4",
   "metadata": {},
   "source": [
    "The `score` function will return the best score among all the references automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cada1a",
   "metadata": {},
   "source": [
    "To understand a text generation system better, we can visualize the matchings in BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcc186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import plot_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ebc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(cands[0], refs[0], lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c1ce5",
   "metadata": {},
   "source": [
    "Similarly, we can apply rescaling to adjust the similarity distribution to be more distinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(cands[0], refs[0], lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407458f",
   "metadata": {},
   "source": [
    "## Object-oriented API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2de316",
   "metadata": {},
   "source": [
    "In practice, most of the time of calling the `score` function is spent on building the model. In situations when we want to call the `score` function repeatedly, it is better to cache the model in a `scorer` object. Hence, in `bert_score` we also provide an object-oriented API. \n",
    "\n",
    "The `BERTScorer` class provides the two methods we have introduced above, `score` and `plot_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf25c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = scorer.score(cands, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5161e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.plot_example(cands[0], refs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a087733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
