{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8674a243",
   "metadata": {},
   "source": [
    "### HuggingFace Semantic Similarity Estimation\n",
    "What are we Doing? \n",
    "This is essentially a fact finding experiment. \n",
    "\n",
    "We want to estimate the possibility of using the current corpus as part of a model to evaluate potential future articles\n",
    "\n",
    "<div>\n",
    "   <img src=\"../../images/SemanticEstimation.png\" height=\"374\"; width=\"374\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b91e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path : ['/home/deleidos/Notebooks/Newspaper/SemanticEstimation', '/opt/conda/lib/python38.zip', '/opt/conda/lib/python3.8', '/opt/conda/lib/python3.8/lib-dynload', '', '/opt/conda/lib/python3.8/site-packages', '/opt/conda/lib/python3.8/site-packages/IPython/extensions', '/home/deleidos/.ipython', '/home/deleidos/Notebooks/lib']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# LIBRARY_PATH = '/home/deleidos/Notebooks/Newspaper/SemanticEstimation'\n",
    "LIBRARY_PATH = \"/home/deleidos/Notebooks/lib\"\n",
    "module_path = os.path.abspath(os.path.join(LIBRARY_PATH))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print (f\"sys.path : {sys.path}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec60b663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell imports done @ 2023-03-03 03:48:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deleidos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install sentence_transformers\n",
    "import torch\n",
    "\n",
    "import os, csv\n",
    "from csv import writer, DictWriter\n",
    "from os import listdir\n",
    "import os.path as osp\n",
    "from os.path import isfile, isdir, join\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime as DT\n",
    "import json\n",
    "import random\n",
    "# setup widget import\n",
    "from ipywidgets import widgets,Layout\n",
    "from IPython.display import display\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('brown')\n",
    "# Locals \n",
    "from pathlib import Path \n",
    "import os, sys \n",
    "import glob\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# nltk.download('punkt')\n",
    "###\n",
    "module_path = str(Path.cwd().parents[0] / \"\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)   \n",
    "from utils import *\n",
    "import random\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime as DT\n",
    "from os import listdir\n",
    "import os.path as osp\n",
    "from os.path import isfile, join\n",
    "\n",
    "# setup widget import\n",
    "from ipywidgets import widgets,Layout\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dynamoDButils import *\n",
    "# regex\n",
    "import re\n",
    "p = re.compile('[a-z]+')\n",
    "p\n",
    "re.compile('[a-z]+')\n",
    "## LOCAL \n",
    "CSVPath = \"/home/deleidos/Notebooks/CSVData\"\n",
    "JSONPath = \"/home/deleidos/Notebooks/JSONData\"\n",
    "from APIutils import *\n",
    "\n",
    "###\n",
    "def complete(state=\"complete\"):\n",
    "    print(f\"\\nCell {state} @ {(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))}\")\n",
    "complete(state='imports done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7dc09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : us-east-1\n",
      "\n",
      "\n",
      "connecting ...\n",
      "Table {table.name} doesn't exist.\n",
      "\n",
      "Cell DB read @ 2023-03-03 03:48:32\n",
      "CPU times: user 86.6 ms, sys: 14 ms, total: 101 ms\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "RESOURCE = 'dynamodb'\n",
    "TABLE = \"Articles-tfoo6wfq2zbxfdwl3z47nsgtrq-dev\"\n",
    "TABLE = \"Articles-tfoo6wfq2zbxfdwl3z47nsgtrq-dev\"\n",
    "REGION = \"us-east-1\"\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION']= region\n",
    "print (f\"env : {os.getenv('AWS_DEFAULT_REGION')}\")\n",
    "print('\\n\\nconnecting ...')\n",
    "exists = table_exists(RESOURCE, REGION, TABLE)\n",
    "complete(state= \"DB read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594d046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resp = extendedScanArticlesWithKey(RESOURCE,TABLE, columns=\"title,summary\", keyword=\"url\")\n",
    "print (f\"Resp[3400] : {resp[3400]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe89497",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "The format of each returned item resembles:\n",
    "\n",
    "    'summary': {'S': \n",
    "    \"Stock futures are trading up slightly Monday night as investors tried to shake the previous session's selloff.S&P 500 \n",
    "    futures were fractionally higher while Nasdaq-100 futures fell marginally.Monday's close marked a negative start to the \n",
    "    trading week.Market observers are still largely expecting a 50 basis point increase to interest rates at the Fed's \n",
    "    December meeting.Investors will look ahead to data Tuesday morning on international trade for insight into the strength \n",
    "    of the U.S. and global economy.\"}, \n",
    "    'url': {'S': 'https://www.cnbc.com/2022/12/05/stock-market-futures-open-to-close-news.html'}, \n",
    "    'title': {'S': 'Stock futures are up slightly as fears mount over upcoming interest rate hikes'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0c954",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for item in resp[:2]:  # print  first 10 articles \n",
    "#     print (f\"URL: {item['url']['S']}\\nTitle: {item['title']['S']}:\\nSummary: {item['summary']['S']}\", end=\"\\n\\n\")\n",
    "# # TODO:\n",
    "# # clean up the return s/t it does not have stuff like urls or (money, per centage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6885d",
   "metadata": {},
   "source": [
    "#####  Write the return items into a JSON  file s/t we can later read back in and use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19338ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json\n",
    "indx = 0\n",
    "jsndict = {}\n",
    "for item in resp:\n",
    "    jsndict[str(indx)]= {\n",
    "        \"url\" : item['url']['S'],\n",
    "        \"title\" : item['title']['S'],\n",
    "        \"summary\": item['summary']['S']\n",
    "    }\n",
    "    indx += 1\n",
    "    \n",
    "json_object = json.dumps(jsndict, indent=4)\n",
    "jsonfile = join(JSONPATH, \"semanticSimArticlesME.json\")\n",
    "\n",
    "\n",
    "with open(jsonfile, \"w\") as outfile:\n",
    "     outfile.write(json_object)\n",
    "complete(state=\"JSON created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ecb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles = [f for f in listdir(JSONPATH) if isfile(join(JSONPATH, f))]\n",
    "jsonfiles.sort()\n",
    "jsonfiles = [x for x in jsonfiles if (os.path.splitext(x)[1][1:]).lower() == \"json\"]\n",
    "jsnWD = widgets.Select(\n",
    "    options=jsonfiles,\n",
    "    value=jsonfiles[0],\n",
    "    description='Choose ID:',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        layout=Layout(width='50%', height='256px')\n",
    ")\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "#         print (\"changed to %s\" % change['new'])\n",
    "        selection = change['new']\n",
    "\n",
    "jsnWD.observe(on_change)\n",
    "display(jsnWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdf2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsnf = jsnWD.value\n",
    "print (f\"{jsnf=} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b40ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsnfile = osp.join (JSONPATH,jsnf)\n",
    "df = pd.read_json(jsnfile)\n",
    "xposedf = df.transpose() # pivot the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "xposedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = xposedf.summary.to_list()\n",
    "# print(f\"Summaries-first: {summaries[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d050fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "complete(state=\"SentenceTransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = summaries\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "complete(state=\"model.encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a54b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#START HERE\n",
    "### read in the target summaries to evaluate against\n",
    "tgtfiles = [f for f in listdir(MODELPATH) if isfile(join(MODELPATH, f))]\n",
    "tgtfiles.sort()\n",
    "tgtfiles = [x for x in tgtfiles if (os.path.splitext(x)[1][1:]).lower() == \"tgt\"]\n",
    "tgtWD = widgets.Select(\n",
    "    options=tgtfiles,\n",
    "    value=tgtfiles[0],\n",
    "    description='Choose File:',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        layout=Layout(width='50%', height='256px')\n",
    ")\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "#         print (\"changed to %s\" % change['new'])\n",
    "        selection = change['new']\n",
    "\n",
    "tgtWD.observe(on_change)\n",
    "display(tgtWD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tgtf =  tgtWD.value\n",
    "print (f\"{tgtf=} \")\n",
    "\n",
    "tgtfile = osp.join (MODELPATH,tgtf)\n",
    "df = pd.read_csv(tgtfile, sep=\"^\")\n",
    "# df = df['ABSTRACTS']\n",
    "iter = 0\n",
    "for index, row in df.iterrows():\n",
    "#     print(row['c1'], row['c2'])\n",
    "    iter += 1\n",
    "    row['SOURCE'] = row['SOURCE'] + str(iter)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert summaries to a list\n",
    "queries = df[\"SUMMARY\"].tolist()\n",
    "print(queries[0])\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries[:1]:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1],):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Make dictionary of lists of the scores for each entry\n",
    "hf_score = {'hf1': [], 'hf2': [], 'hf3': [], 'hf4': [],'hf5': []}\n",
    "\n",
    "# Find the closest 5 entries of the corpus for each query entry based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "    \n",
    "    top_r = torch.topk(cos_scores, k=top_k)\n",
    "    scores = top_results[0].cpu().data.numpy()\n",
    "    indices = top_results[1].cpu().data.numpy()\n",
    "\n",
    "    elt = 0\n",
    "    for score, idx in zip(scores, indices):\n",
    "        score = round(score, 3) # move to cpu memory\n",
    "        elt += 1\n",
    "        hf_score['hf'+str(elt)].append(score)\n",
    "\n",
    "complete(state=\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a df of the HF scores collected\n",
    "hf_df = pd.DataFrame(hf_score)\n",
    "hf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139e596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make a new master df with the source, summary, and corresponding hf scores\n",
    "mixed_df = pd.concat([df, hf_df], axis=1, join='inner')\n",
    "mixed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(test_list):\n",
    "    q1 = np.percentile(test_list, 25)\n",
    "    q3 = np.percentile(test_list, 75)\n",
    "    iqr = (q3 - q1) * 1.5\n",
    "    q_set = (q1 - iqr, q3 + iqr)\n",
    "\n",
    "    result_list =[]\n",
    "\n",
    "    for num in test_list:\n",
    "        if num >= q_set[0] and num <= q_set[1]:\n",
    "            result_list.append(num)\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateScoreAvg(df):\n",
    "    score_avg = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        row_list = df.loc[i, ['hf1', 'hf2', 'hf3', 'hf4', 'hf5']].values.flatten().tolist()\n",
    "        test_list = list(map(float, row_list))\n",
    "        \n",
    "        new_list = removeOutliers(test_list)\n",
    "        weighted_score = np.mean(new_list)\n",
    "#         print(f\"Weighted score of row {i}: {weighted_score}\")\n",
    "        \n",
    "        score_avg.append(weighted_score)\n",
    "        i +=1\n",
    "        \n",
    "    return score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b541e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_avg = calculateScoreAvg(mixed_df)\n",
    "\n",
    "mixed_df['hfAvg'] = hf_avg\n",
    "mixed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a437c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df.rename(columns = {'SOURCE':'source', 'SUMMARY':'summary'}, inplace = True)\n",
    "mixed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = join(JSONPath, \"semanticSimArticlesTestingME.json\")\n",
    "print(jsonfile)\n",
    "\n",
    "mixed_df.to_json(jsonfile)\n",
    "complete(state=\"updates written: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3672fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Make a list of tuples with your data and then create a DataFrame with it:\n",
    "# site = 'mixed-entries'\n",
    "# jsndict = {}\n",
    "# jsndict[site] =  {\"summary\" : summary}\n",
    "# entries = {\"scores\" : {}}\n",
    "\n",
    "# top_r = torch.topk(cos_scores, k=top_k)\n",
    "# scores = top_results[0].cpu().data.numpy()\n",
    "# indices = top_results[1].cpu().data.numpy()\n",
    "\n",
    "# elt = 0\n",
    "# for score, idx in zip(scores, indices):\n",
    "#     score = round(score, 3) # move to cpu memory\n",
    "#     entry = {\n",
    "#         \"corpusindex\" : str(idx),\n",
    "#         \"score\" : str(score),\n",
    "#         \"corpusURL\" : xposedf.iloc[idx].url,\n",
    "#         \"corpusSummary\" : xposedf.iloc[idx].summary,\n",
    "#     }\n",
    "#     elt += 1\n",
    "#     entries['scores'].update({'HF'+str(elt) : entry})\n",
    "    \n",
    "# jsndict[site].update(entries)\n",
    "\n",
    "# # print (f\"results: {jsndict}\")\n",
    "\n",
    "# complete(state=\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046382e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonfile = join(JSONPath, \"semanticSimArticlesTestingME.json\")\n",
    "# # read this in as an array\n",
    "# element = {}\n",
    "# try:\n",
    "#     with open(jsonfile, \"r\") as infile:\n",
    "#         element = json.load(infile)\n",
    "# except FileNotFoundError as e:\n",
    "#     print (f\"Error : {e}. Creating a new list\")\n",
    "    \n",
    "\n",
    "# element.update(jsndict)\n",
    "\n",
    "# print (len(element.keys()))\n",
    "# json_object = json.dumps(element, indent=4)\n",
    "\n",
    "# with open(jsonfile, \"w\") as outfile:\n",
    "#      outfile.write(json_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d94dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
