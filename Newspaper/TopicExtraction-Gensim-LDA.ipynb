{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bddfeff",
   "metadata": {},
   "source": [
    "#### The motivating system diagramme:\n",
    "![Topics](../../Notebooks/images/Topic-Extraction-Modeling-Persistence.jpg)\n",
    "\n",
    "One question I'd like to answer is where (and if) we might get some readymade **Inovation-Centric** datasets _rather than by doing this all by hand_ which would ential combing through the set of articles in our JSON collection.\n",
    "\n",
    "Places we have found:\n",
    "* [\"USPTO Brief Summary Text\"](https://patentsview.org/download/brf_sum_text)\n",
    "* [\"USPTO Detailed Summary Text\"](https://patentsview.org/download/detail_desc_text)\n",
    "* [National Bureau of Economic Rsearch Innovation and R&D\"](https://www.nber.org/taxonomy/term/656?page=1&perPage=100)\n",
    "*[\"Online Appendix to Technological Innovation,Resource Allocation and Growth\"](https://mitsloan.mit.edu/shared/ods/documents?PublicationDocumentID=5894)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae658bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../Notebooks/images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb4080",
   "metadata": {},
   "source": [
    "### Some experimentation to do topic detect and associated keywords\n",
    "#### Latent Dirichlet Allocation\n",
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial.\n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution.\n",
    "\n",
    "Used this as [\"spirit guide\"](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925) and also [this](https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb) and [this](https://scikit-learn.org/stable/datasets/real_world.html) are useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d983ea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/deleidos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from pprint import pprint\n",
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f73721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fetch_20newsgroups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdca5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train number: 5309\n",
      "Test number: 3534\n",
      "CPU times: user 461 ms, sys: 58.3 ms, total: 520 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "categories = [\n",
    "'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x', \n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space'\n",
    "]\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',   categories=categories, shuffle = True)\n",
    "\n",
    "print(f\"Train number: {len(newsgroups_train['data'])}\")\n",
    "print(f\"Test number: {len(newsgroups_test['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87064ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(newsgroups_train.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0a5ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range (4000,4100):\n",
    "#     print(f\"newsgroups_test : {i} :\\n{newsgroups_test['data'][6]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93652dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5309,) (5309,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca97ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ec0242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.02976078357506"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components \n",
    "# by sample in a more than 30000-dimensional space (less than .5% non-zero features):\n",
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736b571",
   "metadata": {},
   "source": [
    "Note 1: \n",
    "[\"sklearn.datasets.fetch_20newsgroups_vectorized\"](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized) is a function which returns ready-to-use tfidf features instead of file names.\n",
    "\n",
    "Note 2:\n",
    "It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that aren’t from this window of time.\n",
    "For this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. **remove** should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36911db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_metrics = 0.8693128151003656\n",
      "train_metrics = 0.9982998470701069\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=.01)\n",
    "newsgroups_test = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "vectors_train= vectorizer.transform(newsgroups_train.data)\n",
    "\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred_test = clf.predict(vectors_test)\n",
    "test_metrics = metrics.f1_score(pred_test, newsgroups_test.target, average='macro')\n",
    "print (f\"test_metrics = {test_metrics}\")\n",
    "\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred_train = clf.predict(vectors_train)\n",
    "train_metrics = metrics.f1_score(pred_train, newsgroups_train.target, average='macro')\n",
    "print (f\"train_metrics = {train_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a2d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics: edu it for in is graphics and of to the\n",
      "comp.os.ms-windows.misc: for in of is and edu it to windows the\n",
      "comp.sys.ibm.pc.hardware: edu for is drive of it scsi and to the\n",
      "comp.sys.mac.hardware: in it is apple and of mac edu to the\n",
      "comp.windows.x: it com motif in is and of window to the\n",
      "sci.crypt: it in clipper that is and key of to the\n",
      "sci.electronics: for it edu you in is and of to the\n",
      "sci.med: pitt edu that it in and is to of the\n",
      "sci.space: edu nasa that is and in space to of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3df7",
   "metadata": {},
   "source": [
    "## OLDER experimental code\n",
    "### Let's assume that\n",
    "\n",
    "    1. user has selected an article\n",
    "    2. we will use the summary to gain some understanding of the topic\n",
    "    3. we can then present the topics to launch a new search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea61accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2fad34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical summary from our news feeds\n",
    "summary = \"\"\"\n",
    "ShotSpotter evidence has increasingly been admitted in court cases around the country, now totaling some 200.\\n\n",
    "But even as its use has expanded in court, ShotSpotter’s technology has drawn scrutiny.\\n\n",
    "Some courts, too, have been less than impressed with the ShotSpotter system.\\n\n",
    "Nonetheless, the split three-judge panel concluded that other evidence prosecutors presented was enough to uphold Godinez’s conviction.\\n\n",
    "Max, who requested it, said such material could be used to cast doubt on the validity and reliability of ShotSpotter evidence in cases nationwide.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a537f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['', 'ShotSpotter', 'evidence', 'has', 'increasingly', 'been', 'admitted', 'in', 'court', 'cases', 'around', 'the', 'country,', 'now', 'totaling', 'some', '200.', '', 'But', 'even', 'as', 'its', 'use', 'has', 'expanded', 'in', 'court,', 'ShotSpotter’s', 'technology', 'has', 'drawn', 'scrutiny.', '', 'Some', 'courts,', 'too,', 'have', 'been', 'less', 'than', 'impressed', 'with', 'the', 'ShotSpotter', 'system.', '', 'Nonetheless,', 'the', 'split', 'three-judge', 'panel', 'concluded', 'that', 'other', 'evidence', 'prosecutors', 'presented', 'was', 'enough', 'to', 'uphold', 'Godinez’s', 'conviction.', '', 'Max,', 'who', 'requested', 'it,', 'said', 'such', 'material', 'could', 'be', 'used', 'to', 'cast', 'doubt', 'on', 'the', 'validity', 'and', 'reliability', 'of', 'ShotSpotter', 'evidence', 'in', 'cases', 'nationwide.', '']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['shotspott', 'evid', 'increas', 'admit', 'court', 'case', 'countri', 'total', 'expand', 'court', 'shotspott', 'technolog', 'draw', 'scrutini', 'court', 'impress', 'shotspott', 'nonetheless', 'split', 'judg', 'panel', 'conclud', 'evid', 'prosecutor', 'present', 'uphold', 'godinez', 'convict', 'request', 'say', 'materi', 'cast', 'doubt', 'valid', 'reliabl', 'shotspott', 'evid', 'case', 'nationwid']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original document: \")\n",
    "words = []\n",
    "summary  = summary.replace(\"\\n\", \" \")\n",
    "\n",
    "for word in summary.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae8c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "processed_docs.append(preprocess(summary))\n",
    "\n",
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e86088b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 admit\n",
      "1 case\n",
      "2 cast\n",
      "3 conclud\n",
      "4 convict\n",
      "5 countri\n",
      "6 court\n",
      "7 doubt\n",
      "8 draw\n",
      "9 evid\n",
      "10 expand\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571b5cf",
   "metadata": {},
   "source": [
    "#### Gensim filter_extremes\n",
    "\n",
    "    filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "    * less than no_below documents (absolute number) or\n",
    "    * more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "    * after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "934da9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f2184",
   "metadata": {},
   "source": [
    "#### Gensim doc2bow\n",
    "\n",
    "doc2bow(document)\n",
    "\n",
    "  * Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a46d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "623692a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 0\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbc346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
