{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bddfeff",
   "metadata": {},
   "source": [
    "#### The motivating system diagramme:\n",
    "![Topics](../../Notebooks/images/Topic-Extraction-Modeling-Persistence.jpg)\n",
    "\n",
    "One question I'd like to answer is where (and if) we might get some readymade **Inovation-Centric** datasets _rather than by doing this all by hand_ which would ential combing through the set of articles in our JSON collection.\n",
    "\n",
    "Places we have found:\n",
    "* [\"USPTO Brief Summary Text\"](https://patentsview.org/download/brf_sum_text)\n",
    "* [\"USPTO Detailed Summary Text\"](https://patentsview.org/download/detail_desc_text)\n",
    "* [National Bureau of Economic Rsearch Innovation and R&D\"](https://www.nber.org/taxonomy/term/656?page=1&perPage=100)\n",
    "*[\"Online Appendix to Technological Innovation,Resource Allocation and Growth\"](https://mitsloan.mit.edu/shared/ods/documents?PublicationDocumentID=5894)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae658bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../Notebooks/images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb4080",
   "metadata": {},
   "source": [
    "### Some experimentation to do topic detect and associated keywords\n",
    "#### Latent Dirichlet Allocation\n",
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial.\n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution.\n",
    "\n",
    "Used this as [\"spirit guide\"](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925) and also [this](https://github.com/priya-dwivedi/Deep-Learning/blob/master/topic_modeling/LDA_Newsgroup.ipynb) and [this](https://scikit-learn.org/stable/datasets/real_world.html) are useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d983ea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/deleidos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from pprint import pprint\n",
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f73721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fetch_20newsgroups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdca5521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train number: 5309\n",
      "Test number: 3534\n",
      "CPU times: user 461 ms, sys: 58.3 ms, total: 520 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "categories = [\n",
    "'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x', \n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space'\n",
    "]\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',   categories=categories, shuffle = True)\n",
    "\n",
    "print(f\"Train number: {len(newsgroups_train['data'])}\")\n",
    "print(f\"Test number: {len(newsgroups_test['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c87064ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(newsgroups_train.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0a5ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range (4000,4100):\n",
    "#     print(f\"newsgroups_test : {i} :\\n{newsgroups_test['data'][6]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93652dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5309,) (5309,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca97ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ec0242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.02976078357506"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero components \n",
    "# by sample in a more than 30000-dimensional space (less than .5% non-zero features):\n",
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736b571",
   "metadata": {},
   "source": [
    "Note 1: \n",
    "[\"sklearn.datasets.fetch_20newsgroups_vectorized\"](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized) is a function which returns ready-to-use tfidf features instead of file names.\n",
    "\n",
    "Note 2:\n",
    "It is easy for a classifier to overfit on particular things that appear in the 20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very high F-scores, but their results would not generalize to other documents that arenâ€™t from this window of time.\n",
    "For this reason, the functions that load 20 Newsgroups data provide a parameter called remove, telling it what kinds of information to strip out of each file. **remove** should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36911db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_metrics = 0.8693128151003656\n",
      "train_metrics = 0.9982998470701069\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=.01)\n",
    "newsgroups_test = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "vectors_train= vectorizer.transform(newsgroups_train.data)\n",
    "\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred_test = clf.predict(vectors_test)\n",
    "test_metrics = metrics.f1_score(pred_test, newsgroups_test.target, average='macro')\n",
    "print (f\"test_metrics = {test_metrics}\")\n",
    "\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred_train = clf.predict(vectors_train)\n",
    "train_metrics = metrics.f1_score(pred_train, newsgroups_train.target, average='macro')\n",
    "print (f\"train_metrics = {train_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a2d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics: edu it for in is graphics and of to the\n",
      "comp.os.ms-windows.misc: for in of is and edu it to windows the\n",
      "comp.sys.ibm.pc.hardware: edu for is drive of it scsi and to the\n",
      "comp.sys.mac.hardware: in it is apple and of mac edu to the\n",
      "comp.windows.x: it com motif in is and of window to the\n",
      "sci.crypt: it in clipper that is and key of to the\n",
      "sci.electronics: for it edu you in is and of to the\n",
      "sci.med: pitt edu that it in and is to of the\n",
      "sci.space: edu nasa that is and in space to of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "\n",
    "show_top10(clf, vectorizer, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc3df7",
   "metadata": {},
   "source": [
    "## OLDER experimental code\n",
    "### Let's assume that\n",
    "\n",
    "    1. user has selected an article\n",
    "    2. we will use the summary to gain some understanding of the topic\n",
    "    3. we can then present the topics to launch a new search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea61accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2fad34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical summary from our news feeds\n",
    "summary = \"\"\"\n",
    "ShotSpotter evidence has increasingly been admitted in court cases around the country, now totaling some 200.\\n\n",
    "But even as its use has expanded in court, ShotSpotterâ€™s technology has drawn scrutiny.\\n\n",
    "Some courts, too, have been less than impressed with the ShotSpotter system.\\n\n",
    "Nonetheless, the split three-judge panel concluded that other evidence prosecutors presented was enough to uphold Godinezâ€™s conviction.\\n\n",
    "Max, who requested it, said such material could be used to cast doubt on the validity and reliability of ShotSpotter evidence in cases nationwide.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a537f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['', 'ShotSpotter', 'evidence', 'has', 'increasingly', 'been', 'admitted', 'in', 'court', 'cases', 'around', 'the', 'country,', 'now', 'totaling', 'some', '200.', '', 'But', 'even', 'as', 'its', 'use', 'has', 'expanded', 'in', 'court,', 'ShotSpotterâ€™s', 'technology', 'has', 'drawn', 'scrutiny.', '', 'Some', 'courts,', 'too,', 'have', 'been', 'less', 'than', 'impressed', 'with', 'the', 'ShotSpotter', 'system.', '', 'Nonetheless,', 'the', 'split', 'three-judge', 'panel', 'concluded', 'that', 'other', 'evidence', 'prosecutors', 'presented', 'was', 'enough', 'to', 'uphold', 'Godinezâ€™s', 'conviction.', '', 'Max,', 'who', 'requested', 'it,', 'said', 'such', 'material', 'could', 'be', 'used', 'to', 'cast', 'doubt', 'on', 'the', 'validity', 'and', 'reliability', 'of', 'ShotSpotter', 'evidence', 'in', 'cases', 'nationwide.', '']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['shotspott', 'evid', 'increas', 'admit', 'court', 'case', 'countri', 'total', 'expand', 'court', 'shotspott', 'technolog', 'draw', 'scrutini', 'court', 'impress', 'shotspott', 'nonetheless', 'split', 'judg', 'panel', 'conclud', 'evid', 'prosecutor', 'present', 'uphold', 'godinez', 'convict', 'request', 'say', 'materi', 'cast', 'doubt', 'valid', 'reliabl', 'shotspott', 'evid', 'case', 'nationwid']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original document: \")\n",
    "words = []\n",
    "summary  = summary.replace(\"\\n\", \" \")\n",
    "\n",
    "for word in summary.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae8c666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "processed_docs.append(preprocess(summary))\n",
    "\n",
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e86088b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 admit\n",
      "1 case\n",
      "2 cast\n",
      "3 conclud\n",
      "4 convict\n",
      "5 countri\n",
      "6 court\n",
      "7 doubt\n",
      "8 draw\n",
      "9 evid\n",
      "10 expand\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking dictionary created\n",
    "'''\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571b5cf",
   "metadata": {},
   "source": [
    "#### Gensim filter_extremes\n",
    "\n",
    "    filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "\n",
    "Filter out tokens that appear in\n",
    "\n",
    "    * less than no_below documents (absolute number) or\n",
    "    * more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "    * after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "934da9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "'''\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f2184",
   "metadata": {},
   "source": [
    "#### Gensim doc2bow\n",
    "\n",
    "doc2bow(document)\n",
    "\n",
    "  * Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a46d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to 'bow_corpus'\n",
    "'''\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "623692a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "document_num = 0\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fbc346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
