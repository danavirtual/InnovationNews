{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e97afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "import contractions\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from utils import *\n",
    "\n",
    "ESHOST = \"ec2-54-90-163-248.compute-1.amazonaws.com\" # ElasticSearch host. Where articles get stored\n",
    "ESPORT = 9200 # ElasticSearch port. Where articles get stored\n",
    "client = ESConnect(host=ESHOST, port=ESPORT) # get all titles from the ES dataset\n",
    "titles = getTitles(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f7aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "sum_list=[]\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    sum_list.append(getSummary(client, titles[i]['label']))\n",
    "df = pd.DataFrame(sum_list, columns =['text']) \n",
    "\n",
    "# remove protocols\n",
    "df.text = df.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text).lower(), 1)\n",
    "\n",
    "# remove non-letters\n",
    "df.text = df.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.text).split()), 1)\n",
    "\n",
    "# convert to lowercase\n",
    "df.text = df.text.apply(lambda x: ' '.join([w.lower() for w in x.split()]))\n",
    "\n",
    "# expand contractions  \n",
    "df.text = df.text.apply(lambda x: ' '.join([contractions.fix(w) for w in x.split()]))\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "df.text = df.text.apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "# remove short words\n",
    "df.text = df.text.apply(lambda x: ' '.join([w.strip() for w in x.split() if len(w.strip()) >= 3]))\n",
    "\n",
    "# lemmatize\n",
    "df.text = df.text.apply(lambda x: ' '.join([WordNetLemmatizer().lemmatize(w) for w in x.split()]))\n",
    "\n",
    "docs = df.text.to_list()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2536a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "# Load sentence transformer model\n",
    "sentence_model = SentenceTransformer(\"all-distilroberta-v1\")\n",
    "\n",
    "# Create documents embeddings\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Define UMAP model to reduce embeddings dimension\n",
    "umap_model = umap.UMAP(n_neighbors=8,\n",
    "                       n_components=5,\n",
    "                       min_dist=0.0,\n",
    "                       metric='cosine',\n",
    "                       low_memory=False)\n",
    "\n",
    "# Define HDBSCAN model to perform documents clustering\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=3,\n",
    "                                min_samples=1,\n",
    "                                metric='euclidean',\n",
    "                                cluster_selection_method='eom',\n",
    "                                prediction_data=True)\n",
    "\n",
    "# Create BERTopic model\n",
    "topic_model = BERTopic(top_n_words=8,\n",
    "                       n_gram_range=(1,3), \n",
    "                       calculate_probabilities=True,\n",
    "                       umap_model= umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       verbose=True)\n",
    "\n",
    "# Train model, extract topics and probabilities\n",
    "topics, probabilities = topic_model.fit_transform(docs, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443dc0b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2d432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get docs from topic\n",
    "\n",
    "df = pd.DataFrame({'topic': topics, 'document': docs})\n",
    "docs_in_topic = df[df.topic == 18]\n",
    "docs_in_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae74f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41395499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
